{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "title"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "overview:automl"
   },
   "source": [
    "This notebook demonstrates how to use the Vertex SDK to create video action recognition models using [AutoML](https://cloud.google.com/vertex-ai/docs/start/automl-users).\n",
    "\n",
    "Specifically, we will be training and evaluating a model that recognizes volleyball serves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dataset:golf,var"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using a [dataset](https://docs.google.com/spreadsheets/d/1MhT_ck2PhwYhPiKBE8v0z2TaFOgSLBgO1v562qObRvc/edit#gid=0) provided by Nagra. It consists of 23 videos, each containing around 50 - 200 serving events.\n",
    "\n",
    "These videos are stored in a [GCS Bucket](https://console.cloud.google.com/storage/browser/aliz_action_recognition_poc/trunc_video?project=sharp-leaf-344111&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "objective:automl,training,batch_prediction"
   },
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook, we create an AutoML video action recognition model from a Python script.\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex `Dataset` resource.\n",
    "- Train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "costs"
   },
   "source": [
    "## Costs\n",
    "\n",
    "This project uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI [(Vertex AI\n",
    "pricing)](https://cloud.google.com/vertex-ai/pricing)\n",
    "* Cloud Storage [(Cloud Storage\n",
    "pricing)](https://cloud.google.com/storage/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Development environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "setup_local"
   },
   "source": [
    "We'll need the following components:\n",
    "\n",
    "0. ### GCPs:\n",
    "\n",
    "- [Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
    "- [GCP billing](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "- [Enablement of the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "\n",
    "1. ### Basics:\n",
    "- Git\n",
    "- Python 3\n",
    "- pipenv\n",
    "\n",
    "2. ### SDKs:\n",
    "- Cloud Storage SDK\n",
    "- AI Platform SDK\n",
    "\n",
    "3. ###  Data and video analytic libraries :\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- OpenCV\n",
    "- [MoviePy](https://zulko.github.io/moviepy/) for video editing\n",
    "\n",
    "\n",
    "Note: if we are running this notebook on Vertex AI Workbench, our environment already meets the requirements 1 and 2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Authenticate Google Cloud account (for local only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "gcp_authenticate"
   },
   "source": [
    "\n",
    "\n",
    "**If we are using Vertex AI Notebooks**, your environment is already authenticated. Skip this step.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "**Click Create service account**.\n",
    "\n",
    "In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "\n",
    "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# This provides access to our Cloud Storage bucket and lets us submit training jobs and prediction\n",
    "# requests.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Google Cloud Notebook, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "# <font color='red'>Global Parameters</font>\n",
    "\n",
    "<font color='red'> This section sets the global parameters for the downstream sections. Always run the cells in this section even if you're only running the later section (e.g. prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sharp-leaf-344111\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: sharp-leaf-344111\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "else:\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [sharp-leaf-344111] or it does not exist.\n",
      "Are you sure you wish to set property [core/project] to sharp-leaf-344111?\n",
      "\n",
      "Do you want to continue (Y/n)?  ^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only run if it is local\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "region"
   },
   "source": [
    "### Region\n",
    "\n",
    "We can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. To start with, we will be using  `europe-west4` as it is the closest to Nagra. The following regions are also available for Vertex AI:\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "Note: we may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services. (learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All Vertex AI resources created will be prefixed by this preset prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PREFIX = \"volley_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "bucket:mbsdk"
   },
   "source": [
    "When we initialize the Vertex SDK for Python, we specify a Cloud Storage staging bucket, into which all data associated with our dataset and model resources will be saved. Set the name of our GCS below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://aliz_action_recognition_poc\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "create_bucket"
   },
   "source": [
    "**Only if the bucket doesn't already exist**: Run the following cell to create a new Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://aliz_action_recognition_poc/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'aliz_action_recognition_poc' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "validate_bucket"
   },
   "source": [
    "Validate access to the bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "id": "validate_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 376743798  2022-03-31T17:13:44Z  gs://aliz_action_recognition_poc/Aspire_11_vs_AZ_Sky_11_Gold_2022-02-05.mp4#1648746824256424  metageneration=1\r\n",
      " 367656178  2022-03-31T17:13:43Z  gs://aliz_action_recognition_poc/Aspire_15_vs_SG_Elite__2022-02-21.mp4#1648746823872950  metageneration=1\r\n",
      "    101022  2022-04-21T14:59:17Z  gs://aliz_action_recognition_poc/Nagra - training.csv#1650553157400516  metageneration=1\r\n",
      " 771812506  2022-03-22T07:48:19Z  gs://aliz_action_recognition_poc/RVA_16P_vs_OJVA.mp4#1647935299871660  metageneration=1\r\n",
      "     34104  2022-03-23T18:21:05Z  gs://aliz_action_recognition_poc/sample.csv#1648059665898925  metageneration=1\r\n",
      "       173  2022-03-31T18:52:01Z  gs://aliz_action_recognition_poc/test.json#1648752721716317  metageneration=1\r\n",
      "       117  2022-03-31T18:47:27Z  gs://aliz_action_recognition_poc/test.jsonl#1648752447798314  metageneration=1\r\n",
      "       168  2022-04-04T19:29:30Z  gs://aliz_action_recognition_poc/test_1.json#1649100570728275  metageneration=1\r\n",
      "     77825  2022-04-13T21:04:24Z  gs://aliz_action_recognition_poc/training.csv#1649883864251607  metageneration=1\r\n",
      "     94866  2022-04-21T15:13:00Z  gs://aliz_action_recognition_poc/training_expanded.csv#1650553980611494  metageneration=1\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-03-22T07:52:14.495044Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-03-23T06:00:27.232831Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-03-23T18:47:14.088600Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-03-23T20:42:14.135730Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-03-31T18:52:06.582487Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-04-01T09:09:53.042999Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-04-01T09:10:25.800412Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-04-01T09:10:34.814325Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/prediction-volley_20220320155122-2022-04-04T19:29:32.870981Z/\r\n",
      "                                 gs://aliz_action_recognition_poc/trunc_video/\r\n",
      "TOTAL: 10 objects, 1516520757 bytes (1.41 GiB)\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Vertex SDK Initialization\n",
    "\n",
    "Initialize the Vertex SDK for Python for our project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "tutorial_start:automl"
   },
   "source": [
    "# Dataset Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "import_file:u_dataset,csv"
   },
   "source": [
    "Now set the variable `IMPORT_FILES` to the location of the CSV index files in Cloud Storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "id": "import_file:golf,csv,var"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILES = [\n",
    "    \"gs://aliz_action_recognition_poc/training_expanded.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: the csv file should have no header.\n",
    "\n",
    "The columns should be as follows:\n",
    "1. \"test\"/\"train\" \n",
    "2. URL of the video\n",
    "3. Start time window for inspection\n",
    "4. End time window for inspection\n",
    "5. Name of action to be detected\n",
    "6. Timestamp of the action\n",
    "\n",
    "All timestamps should be specified as integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "quick_peek:csv"
   },
   "source": [
    "Let's take a quick pick at the import file specified above. \n",
    "- We count the number of examples by counting the number of rows in the CSV index file  (`wc -l`).\n",
    "- We then peek at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "id": "quick_peek:csv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 893\n",
      "First 10 rows:\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,365\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,380\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,401\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,427\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,459\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,497\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,535\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,568\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,604\n",
      "train,gs://aliz_action_recognition_poc/trunc_video/Aspire_15_vs_SG_Elite__2022-02-21.mp4,0,1033,serve,622\n"
     ]
    }
   ],
   "source": [
    "FILE = IMPORT_FILES[0]\n",
    "\n",
    "count = ! gsutil cat $FILE | wc -l\n",
    "print(\"Number of Examples:\", int(count[0]))\n",
    "\n",
    "print(\"First 10 rows:\")\n",
    "! gsutil cat $FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "create_dataset:video,var"
   },
   "source": [
    "## Creation of `Dataset` resource\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `VideoDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource. We'll use a combination of `TRAINING_TIMESTAMP` and `PREFIX`.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "\n",
    "This operation may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TRAINING_TIMESTAMP = datetime.now().strftime(\"%Y/%m/%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "id": "create_dataset:video,var"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating VideoDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create VideoDataset backing LRO: projects/276875326730/locations/us-central1/datasets/6750803482451640320/operations/120355360274907136\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:VideoDataset created. Resource name: projects/276875326730/locations/us-central1/datasets/6750803482451640320\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this VideoDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.VideoDataset('projects/276875326730/locations/us-central1/datasets/6750803482451640320')\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Importing VideoDataset data: projects/276875326730/locations/us-central1/datasets/6750803482451640320\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Import VideoDataset data backing LRO: projects/276875326730/locations/us-central1/datasets/6750803482451640320/operations/809406103262593024\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:VideoDataset data imported. Resource name: projects/276875326730/locations/us-central1/datasets/6750803482451640320\n",
      "projects/276875326730/locations/us-central1/datasets/6750803482451640320\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.VideoDataset.create(\n",
    "    display_name=PREFIX + TRAINING_TIMESTAMP,\n",
    "    gcs_source=IMPORT_FILES,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.video.action_recognition,\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an AutoML model, we will perform the following two steps:\n",
    "1. create a training pipeline\n",
    "2. run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:video,var"
   },
   "source": [
    "\n",
    "## 1. Create training pipeline\n",
    "\n",
    "An AutoML training pipeline is created with the `AutoMLVideoTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `prediction_type`: The type task to train the model for.\n",
    "  - `classification`: A video classification model.\n",
    "  - `object_tracking`: A video object tracking model.\n",
    "  - `action_recognition`: A video action recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "create_automl_pipeline:video,var"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.AutoMLVideoTrainingJob object at 0x7f74a96fba10>\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.AutoMLVideoTrainingJob(\n",
    "    display_name=PREFIX + TRAINING_TIMESTAMP,\n",
    "    prediction_type=\"action_recognition\",\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:video"
   },
   "source": [
    "## 2. Run the training pipeline\n",
    "\n",
    "Next, you run the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline for this project averages to 2 h 15 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:video"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5080582097941299200?project=276875326730\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:AutoMLVideoTrainingJob projects/276875326730/locations/us-central1/trainingPipelines/5080582097941299200 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=PREFIX + TRAINING_TIMESTAMP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the trained model will be done in the next notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk_automl_video_action_recognition_batch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
